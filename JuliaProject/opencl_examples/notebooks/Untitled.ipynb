{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant bench_kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test data: 32 MB\n",
      "Julia Execution time: 0.0379 seconds\n",
      "====================================================\n",
      "Platform name:    AMD Accelerated Parallel Processing\n",
      "Platform profile: FULL_PROFILE\n",
      "Platform vendor:  Advanced Micro Devices, Inc.\n",
      "Platform version: OpenCL 1.2 AMD-APP (1214.3)\n",
      "----------------------------------------------------\n",
      "Device name: AMD FX(tm)-8300 Eight-Core Processor\n",
      "Device type: cpu\n",
      "Device mem: 7942 MB\n",
      "Device max mem alloc: 2048 MB\n",
      "Device max clock freq: 1400 MHZ\n",
      "Device max compute units: 8\n",
      "Device max work group size: 1024\n",
      "Device max work item size: (1024,1024,1024)\n",
      "Execution time of test: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Result norm: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0511 seconds\n",
      "====================================================\n",
      "Platform name:    NVIDIA CUDA\n",
      "Platform profile: FULL_PROFILE\n",
      "Platform vendor:  NVIDIA Corporation\n",
      "Platform version: OpenCL 1.2 CUDA 7.5.23\n",
      "----------------------------------------------------\n",
      "Device name: GeForce GTX 660\n",
      "Device type: gpu\n",
      "Device mem: 2047 MB\n",
      "Device max mem alloc: 512 MB\n",
      "Device max clock freq: 1071 MHZ\n",
      "Device max compute units: 5\n",
      "Device max work group size: 1024\n",
      "Device max work item size: (1024,1024,64)\n",
      "Execution time of test: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Result norm: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0009 seconds\n"
     ]
    }
   ],
   "source": [
    "using Compat\n",
    "\n",
    "import OpenCL\n",
    "const cl = OpenCL\n",
    "\n",
    "const bench_kernel = \"\n",
    "__kernel void sum(__global const float *a,\n",
    "                  __global const float *b,\n",
    "                  __global float *c)\n",
    "{\n",
    "        int gid = get_global_id(0);\n",
    "        float a_temp;\n",
    "        float b_temp;\n",
    "        float c_temp;\n",
    "\n",
    "        a_temp = a[gid]; // my a element (by global ref)\n",
    "        b_temp = b[gid]; // my b element (by global ref)\n",
    "\n",
    "        c_temp = a_temp+b_temp; // sum of my elements\n",
    "        c_temp = c_temp * c_temp; // product of sums\n",
    "        c_temp = c_temp * (a_temp/2.0f); // times 1/2 my a\n",
    "\n",
    "        c[gid] = c_temp; // store result in global memory\n",
    "}\"\n",
    "\n",
    "function cl_performance(ndatapts::Integer, nworkers::Integer)\n",
    "\n",
    "    @assert ndatapts > 0\n",
    "    @assert nworkers > 0\n",
    "\n",
    "    a = rand(Float32,  ndatapts)\n",
    "    b = rand(Float32,  ndatapts)\n",
    "    c = Array(Float32, ndatapts)\n",
    "\n",
    "    @printf(\"Size of test data: %i MB\\n\", sizeof(a) / 1024 / 1024)\n",
    "\n",
    "    t1 = time()\n",
    "    for i in 1:ndatapts\n",
    "        c_temp = a[i] + b[i]\n",
    "        c_temp = c_temp * c_temp\n",
    "        c[i]   = @compat c_temp * (a[i] / 2f0)\n",
    "    end\n",
    "    t2 = time()\n",
    "\n",
    "    @printf(\"Julia Execution time: %.4f seconds\\n\", t2 - t1)\n",
    "\n",
    "    for platform in cl.platforms()\n",
    "\n",
    "        if platform[:name] == \"Portable Computing Language\"\n",
    "            warn(\"Portable Computing Language platform not yet supported\")\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        for device in cl.available_devices(platform)\n",
    "            @printf(\"====================================================\\n\")\n",
    "            @printf(\"Platform name:    %s\\n\",  platform[:name])\n",
    "            @printf(\"Platform profile: %s\\n\",  platform[:profile])\n",
    "            @printf(\"Platform vendor:  %s\\n\",  platform[:vendor])\n",
    "            @printf(\"Platform version: %s\\n\",  platform[:version])\n",
    "            @printf(\"----------------------------------------------------\\n\")\n",
    "            @printf(\"Device name: %s\\n\", device[:name])\n",
    "            @printf(\"Device type: %s\\n\", device[:device_type])\n",
    "            @printf(\"Device mem: %i MB\\n\",           device[:global_mem_size] / 1024^2)\n",
    "            @printf(\"Device max mem alloc: %i MB\\n\", device[:max_mem_alloc_size] / 1024^2)\n",
    "            @printf(\"Device max clock freq: %i MHZ\\n\",  device[:max_clock_frequency])\n",
    "            @printf(\"Device max compute units: %i\\n\",   device[:max_compute_units])\n",
    "            @printf(\"Device max work group size: %i\\n\", device[:max_work_group_size])\n",
    "            @printf(\"Device max work item size: %s\\n\",  device[:max_work_item_size])\n",
    "\n",
    "            if device[:max_mem_alloc_size] < sizeof(Float32) * ndatapts\n",
    "                warn(\"Requested buffer size exceeds device max alloc size!\")\n",
    "                warn(\"Skipping device $(device[:name])...\")\n",
    "                continue\n",
    "            end\n",
    "\n",
    "            if device[:max_work_group_size] < nworkers\n",
    "                warn(\"Number of workers exceeds the device's max work group size!\")\n",
    "                warn(\"Skipping device $(device[:name])...\")\n",
    "                continue\n",
    "            end\n",
    "\n",
    "            ctx   = cl.Context(device)\n",
    "            queue = cl.CmdQueue(ctx, :profile)\n",
    "\n",
    "            a_buf = cl.Buffer(Float32, ctx, (:r, :copy), hostbuf=a)\n",
    "            b_buf = cl.Buffer(Float32, ctx, (:r, :copy), hostbuf=b)\n",
    "            c_buf = cl.Buffer(Float32, ctx, :w, length(a))\n",
    "\n",
    "            prg  = cl.Program(ctx, source=bench_kernel) |> cl.build!\n",
    "            kern = cl.Kernel(prg, \"sum\")\n",
    "\n",
    "            # work_group_multiple = kern[:prefered_work_group_size_multiple]\n",
    "            global_size = (ndatapts,)\n",
    "            local_size  = (nworkers,)\n",
    "\n",
    "            # call the kernel\n",
    "            evt = kern[queue, global_size, local_size](a_buf, b_buf, c_buf)\n",
    "\n",
    "            # duration in ns\n",
    "            t = evt[:profile_duration] * 1e-9\n",
    "            @printf(\"Execution time of test: %.4f seconds\\n\", t)\n",
    "\n",
    "            c_device = cl.read(queue, c_buf)\n",
    "            info(\"Result norm: $(norm(c - c_device))\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Play with these numbers to see performance differences\n",
    "# N_DATAPTS has to be a multiple of the number of workers\n",
    "# N_WORKERS has to be less than or equal to the device's max work group size\n",
    "# ex. N_WORKERS = 1 is non parallel execution on the gpu\n",
    "\n",
    "const N_DATA_PTS = @compat Int(2^23) # ~8 million\n",
    "const N_WORKERS  = @compat Int(2^7)\n",
    "cl_performance(N_DATA_PTS, N_WORKERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
