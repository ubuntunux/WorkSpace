{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000-element Array{Float32,1}:\n",
       " 0.19873  \n",
       " 1.10052  \n",
       " 0.508313 \n",
       " 1.55512  \n",
       " 1.1508   \n",
       " 0.140978 \n",
       " 1.27777  \n",
       " 1.76801  \n",
       " 1.3206   \n",
       " 1.73559  \n",
       " 0.656078 \n",
       " 1.25466  \n",
       " 1.24801  \n",
       " â‹®        \n",
       " 1.08083  \n",
       " 0.529467 \n",
       " 1.33889  \n",
       " 1.52574  \n",
       " 0.0938004\n",
       " 0.99119  \n",
       " 1.39611  \n",
       " 1.4061   \n",
       " 0.540827 \n",
       " 1.3385   \n",
       " 1.57758  \n",
       " 1.02002  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Compat\n",
    "\n",
    "import OpenCL\n",
    "const cl = OpenCL\n",
    "\n",
    "const bench_kernel = \"\n",
    "__kernel void sum(__global const float *a,\n",
    "                  __global const float *b,\n",
    "                  __global float *c)\n",
    "{\n",
    "        int gid = get_global_id(0);\n",
    "        float a_temp;\n",
    "        float b_temp;\n",
    "        float c_temp;\n",
    "\n",
    "        a_temp = a[gid]; // my a element (by global ref)\n",
    "        b_temp = b[gid]; // my b element (by global ref)\n",
    "\n",
    "        c_temp = a_temp+b_temp; // sum of my elements\n",
    "        c_temp = c_temp * c_temp; // product of sums\n",
    "        c_temp = c_temp * (a_temp/2.0f); // times 1/2 my a\n",
    "\n",
    "        c[gid] = c_temp; // store result in global memory\n",
    "}\"\n",
    "\n",
    "function cl_performance(ndatapts::Integer, nworkers::Integer)\n",
    "\n",
    "    @assert ndatapts > 0\n",
    "    @assert nworkers > 0\n",
    "\n",
    "    a = rand(Float32,  ndatapts)\n",
    "    b = rand(Float32,  ndatapts)\n",
    "    c = Array(Float32, ndatapts)\n",
    "\n",
    "    @printf(\"Size of test data: %i MB\\n\", sizeof(a) / 1024 / 1024)\n",
    "\n",
    "    t1 = time()\n",
    "    for i in 1:ndatapts\n",
    "        c_temp = a[i] + b[i]\n",
    "        c_temp = c_temp * c_temp\n",
    "        c[i]   = @compat c_temp * (a[i] / 2f0)\n",
    "    end\n",
    "    t2 = time()\n",
    "\n",
    "    @printf(\"Julia Execution time: %.4f seconds\\n\", t2 - t1)\n",
    "\n",
    "    for platform in cl.platforms()\n",
    "\n",
    "        if platform[:name] == \"Portable Computing Language\"\n",
    "            warn(\"Portable Computing Language platform not yet supported\")\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        for device in cl.available_devices(platform)\n",
    "            @printf(\"====================================================\\n\")\n",
    "            @printf(\"Platform name:    %s\\n\",  platform[:name])\n",
    "            @printf(\"Platform profile: %s\\n\",  platform[:profile])\n",
    "            @printf(\"Platform vendor:  %s\\n\",  platform[:vendor])\n",
    "            @printf(\"Platform version: %s\\n\",  platform[:version])\n",
    "            @printf(\"----------------------------------------------------\\n\")\n",
    "            @printf(\"Device name: %s\\n\", device[:name])\n",
    "            @printf(\"Device type: %s\\n\", device[:device_type])\n",
    "            @printf(\"Device mem: %i MB\\n\",           device[:global_mem_size] / 1024^2)\n",
    "            @printf(\"Device max mem alloc: %i MB\\n\", device[:max_mem_alloc_size] / 1024^2)\n",
    "            @printf(\"Device max clock freq: %i MHZ\\n\",  device[:max_clock_frequency])\n",
    "            @printf(\"Device max compute units: %i\\n\",   device[:max_compute_units])\n",
    "            @printf(\"Device max work group size: %i\\n\", device[:max_work_group_size])\n",
    "            @printf(\"Device max work item size: %s\\n\",  device[:max_work_item_size])\n",
    "\n",
    "            if device[:max_mem_alloc_size] < sizeof(Float32) * ndatapts\n",
    "                warn(\"Requested buffer size exceeds device max alloc size!\")\n",
    "                warn(\"Skipping device $(device[:name])...\")\n",
    "                continue\n",
    "            end\n",
    "\n",
    "            if device[:max_work_group_size] < nworkers\n",
    "                warn(\"Number of workers exceeds the device's max work group size!\")\n",
    "                warn(\"Skipping device $(device[:name])...\")\n",
    "                continue\n",
    "            end\n",
    "\n",
    "            ctx   = cl.Context(device)\n",
    "            queue = cl.CmdQueue(ctx, :profile)\n",
    "\n",
    "            a_buf = cl.Buffer(Float32, ctx, (:r, :copy), hostbuf=a)\n",
    "            b_buf = cl.Buffer(Float32, ctx, (:r, :copy), hostbuf=b)\n",
    "            c_buf = cl.Buffer(Float32, ctx, :w, length(a))\n",
    "\n",
    "            prg  = cl.Program(ctx, source=bench_kernel) |> cl.build!\n",
    "            kern = cl.Kernel(prg, \"sum\")\n",
    "\n",
    "            # work_group_multiple = kern[:prefered_work_group_size_multiple]\n",
    "            global_size = (ndatapts,)\n",
    "            local_size  = (nworkers,)\n",
    "\n",
    "            # call the kernel\n",
    "            evt = kern[queue, global_size, local_size](a_buf, b_buf, c_buf)\n",
    "\n",
    "            # duration in ns\n",
    "            t = evt[:profile_duration] * 1e-9\n",
    "            @printf(\"Execution time of test: %.4f seconds\\n\", t)\n",
    "\n",
    "            c_device = cl.read(queue, c_buf)\n",
    "            info(\"Result norm: $(norm(c - c_device))\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Play with these numbers to see performance differences\n",
    "# N_DATAPTS has to be a multiple of the number of workers\n",
    "# N_WORKERS has to be less than or equal to the device's max work group size\n",
    "# ex. N_WORKERS = 1 is non parallel execution on the gpu\n",
    "\n",
    "const N_DATA_PTS = @compat Int(2^23) # ~8 million\n",
    "const N_WORKERS  = @compat Int(2^7)\n",
    "cl_performance(N_DATA_PTS, N_WORKERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
